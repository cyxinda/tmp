apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2024-01-23T06:58:37Z"
  generateName: rook-ceph-osd-31-8668454f9b-
  labels:
    app: rook-ceph-osd
    app.kubernetes.io/component: cephclusters.ceph.rook.io
    app.kubernetes.io/created-by: rook-ceph-operator
    app.kubernetes.io/instance: "31"
    app.kubernetes.io/managed-by: rook-ceph-operator
    app.kubernetes.io/name: ceph-osd
    app.kubernetes.io/part-of: rook-ceph
    ceph-osd-id: "31"
    ceph_daemon_id: "31"
    ceph_daemon_type: osd
    device-class: hdd
    failure-domain: sc-node-ceph-4
    osd: "31"
    osd-store: bluestore
    pod-template-hash: 8668454f9b
    portable: "false"
    rook.io/operator-namespace: rook-ceph
    rook_cluster: rook-ceph
    topology-location-host: sc-node-ceph-4
    topology-location-root: default
  name: rook-ceph-osd-31-8668454f9b-rfzgb
  namespace: rook-ceph
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: rook-ceph-osd-31-8668454f9b
    uid: b7b69037-2651-479e-9f61-4ae3101f5185
  resourceVersion: "129751027"
  uid: 4234e6b5-2e92-4d62-8030-9361efeaa988
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: type.knowdee.io
            operator: In
            values:
            - storage
          - key: storage.knowdee.io
            operator: In
            values:
            - ceph
  containers:
  - args:
    - --foreground
    - --id
    - "31"
    - --fsid
    - aa0e7bed-d3e3-49a7-a471-8e354bfe61f6
    - --setuser
    - ceph
    - --setgroup
    - ceph
    - --crush-location=root=default host=sc-node-ceph-4
    - --default-log-to-stderr=true
    - --default-err-to-stderr=true
    - --default-mon-cluster-log-to-stderr=true
    - '--default-log-stderr-prefix=debug '
    - --default-log-to-file=false
    - --default-mon-cluster-log-to-file=false
    command:
    - ceph-osd
    env:
    - name: ROOK_NODE_NAME
      value: sc-node-ceph-4
    - name: ROOK_CLUSTER_ID
      value: 93411de7-56da-4683-8bb8-2a68e18b0987
    - name: ROOK_CLUSTER_NAME
      value: rook-ceph
    - name: ROOK_PRIVATE_IP
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: status.podIP
    - name: ROOK_PUBLIC_IP
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: status.podIP
    - name: POD_NAMESPACE
      value: rook-ceph
    - name: ROOK_MON_ENDPOINTS
      valueFrom:
        configMapKeyRef:
          key: data
          name: rook-ceph-mon-endpoints
    - name: ROOK_CONFIG_DIR
      value: /var/lib/rook
    - name: ROOK_CEPH_CONFIG_OVERRIDE
      value: /etc/rook/config/override.conf
    - name: NODE_NAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: spec.nodeName
    - name: ROOK_CRUSHMAP_ROOT
      value: default
    - name: ROOK_CRUSHMAP_HOSTNAME
      value: sc-node-ceph-4
    - name: CEPH_VOLUME_DEBUG
      value: "1"
    - name: CEPH_VOLUME_SKIP_RESTORECON
      value: "1"
    - name: DM_DISABLE_UDEV
      value: "1"
    - name: ROOK_OSDS_PER_DEVICE
      value: "1"
    - name: CONTAINER_IMAGE
      value: quay.io/ceph/ceph:v17.2.6
    - name: POD_NAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.name
    - name: POD_MEMORY_LIMIT
      valueFrom:
        resourceFieldRef:
          divisor: "0"
          resource: limits.memory
    - name: POD_MEMORY_REQUEST
      valueFrom:
        resourceFieldRef:
          divisor: "0"
          resource: requests.memory
    - name: POD_CPU_LIMIT
      valueFrom:
        resourceFieldRef:
          divisor: "1"
          resource: limits.cpu
    - name: POD_CPU_REQUEST
      valueFrom:
        resourceFieldRef:
          divisor: "0"
          resource: requests.cpu
    - name: CEPH_USE_RANDOM_NONCE
      value: "true"
    - name: ROOK_OSD_RESTART_INTERVAL
      value: "0"
    - name: ROOK_OSD_UUID
      value: c803a2ec-8f5c-4b23-b1d1-1319ea2fd989
    - name: ROOK_OSD_ID
      value: "31"
    - name: ROOK_CEPH_MON_HOST
      valueFrom:
        secretKeyRef:
          key: mon_host
          name: rook-ceph-config
    - name: CEPH_ARGS
      value: -m $(ROOK_CEPH_MON_HOST)
    - name: ROOK_BLOCK_PATH
      value: /dev/sde
    - name: ROOK_CV_MODE
      value: raw
    - name: ROOK_OSD_DEVICE_CLASS
      value: hdd
    - name: ROOK_POD_IP
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: status.podIP
    - name: ROOK_MSGR2
      value: msgr2_false_encryption_false_compression_false
    envFrom:
    - configMapRef:
        name: rook-ceph-osd-env-override
        optional: true
    image: quay.io/ceph/ceph:v17.2.6
    imagePullPolicy: IfNotPresent
    livenessProbe:
      exec:
        command:
        - env
        - -i
        - sh
        - -c
        - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.31.asok status 2>&1)\"\nrc=$?\nif
          [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
          check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
          /g'\n\texit $rc\nfi\n"
      failureThreshold: 3
      initialDelaySeconds: 45
      periodSeconds: 90
      successThreshold: 1
      timeoutSeconds: 19000
    name: osd
    resources:
      limits:
        memory: 24Gi
      requests:
        cpu: "1"
        memory: 18Gi
    securityContext:
      privileged: true
      readOnlyRootFilesystem: false
      runAsUser: 0
    startupProbe:
      exec:
        command:
        - env
        - -i
        - sh
        - -c
        - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.31.asok status 2>&1)\"\nrc=$?\nif
          [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
          check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
          /g'\n\texit $rc\nfi\n"
      failureThreshold: 720
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 5
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/lib/rook
      name: rook-data
    - mountPath: /etc/ceph
      name: rook-config-override
      readOnly: true
    - mountPath: /run/ceph
      name: ceph-daemons-sock-dir
    - mountPath: /var/log/ceph
      name: rook-ceph-log
    - mountPath: /var/lib/ceph/crash
      name: rook-ceph-crash
    - mountPath: /dev
      name: devices
    - mountPath: /run/udev
      name: run-udev
    - mountPath: /var/lib/ceph/osd/ceph-31
      name: activate-osd
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-p7ngl
      readOnly: true
    workingDir: /var/log/ceph
  - command:
    - /bin/bash
    - -x
    - -e
    - -m
    - -c
    - "\nCEPH_CLIENT_ID=ceph-osd.31\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
      edit the logrotate file to only rotate a specific daemon log\n# otherwise we
      will logrotate log files without reloading certain daemons\n# this might happen
      when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
      \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
      --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
      rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
      \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
      != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the logrotate
      config file with 4 spaces to maintain indentation\n\tsed --in-place \"4i \\
      \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile true;
      do\n\t# we don't force the logrorate but we let the logrotate binary handle
      the rotation based on user's input for periodicity and size\n\tlogrotate --verbose
      \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
    image: quay.io/ceph/ceph:v17.2.6
    imagePullPolicy: IfNotPresent
    name: log-collector
    resources:
      limits:
        memory: 2Gi
      requests:
        cpu: 500m
        memory: 1Gi
    securityContext:
      privileged: false
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    tty: true
    volumeMounts:
    - mountPath: /etc/ceph
      name: rook-config-override
      readOnly: true
    - mountPath: /run/ceph
      name: ceph-daemons-sock-dir
    - mountPath: /var/log/ceph
      name: rook-ceph-log
    - mountPath: /var/lib/ceph/crash
      name: rook-ceph-crash
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-p7ngl
      readOnly: true
  dnsPolicy: ClusterFirstWithHostNet
  enableServiceLinks: true
  hostNetwork: true
  initContainers:
  - command:
    - /bin/bash
    - -c
    - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables are unset\nset
      -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=aa0e7bed-d3e3-49a7-a471-8e354bfe61f6\nOSD_UUID=c803a2ec-8f5c-4b23-b1d1-1319ea2fd989\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
      \"ceph.conf\" must have the \"fsid\" global configuration to activate encrypted
      OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
      This limitation will be removed later. After that, we can remove this\n# fsid
      injection code. Probably a good time is when to remove Quincy support.\n# https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp
      --no-preserve=mode /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c
      \"\nimport configparser\n\nconfig = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
      not config.has_section('global'):\n    config['global'] = {}\n\nif not config.has_option('global','fsid'):\n
      \   config['global']['fsid'] = '$CEPH_FSID'\n\nwith open('/etc/ceph/ceph.conf',
      'w') as configfile:\n    config.write(configfile)\n\"\n\n# create new keyring\nceph
      -n client.admin auth get-or-create osd.\"$OSD_ID\" mon 'allow profile osd' mgr
      'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
      active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
      -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
      \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary directory\n\t#
      this is needed because when the init container exits, the tmpfs goes away and
      its content with it\n\t# this will result in the emptydir to be empty when accessed
      by the main osd container\n\tcp --verbose --no-dereference \"$OSD_DATA_DIR\"/*
      \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't need it anymore\n\tumount
      \"$OSD_DATA_DIR\"\n\n\t# copy back the content of the tmpfs into the original
      osd directory\n\tcp --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t#
      retain ownership of files to the ceph user/group\n\tchown --verbose --recursive
      ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm --recursive
      --force \"$TMP_DIR\"\nelse\n\t# 'ceph-volume raw list' (which the osd-prepare
      job uses to report OSDs on nodes)\n\t#  returns user-friendly device names which
      can change when systems reboot. To\n\t# keep OSD pods from crashing repeatedly
      after a reboot, we need to check if the\n\t# block device we have is still correct,
      and if it isn't correct, we need to\n\t# scan all the disks to find the right
      one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t# jq would
      be preferable, but might be removed for hardened Ceph images\n\t\t# python3
      should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport sys, json\nfor
      _, info in json.load(sys.stdin).items():\n\tif info['osd_id'] == $OSD_ID:\n\t\tprint(info['device'],
      end='')\n\t\tprint('found device: ' + info['device'], file=sys.stderr) # log
      the disk we found to stderr\n\t\tsys.exit(0)  # don't keep processing once the
      disk is found\nsys.exit('no disk found with OSD ID $OSD_ID')\n\"\n\t}\n\n\tceph-volume
      raw list \"$DEVICE\" > \"$OSD_LIST\"\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device
      < \"$OSD_LIST\"; then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device
      < \"$OSD_LIST\")\"\n\tfi\n\t[[ -z \"$DEVICE\" ]] && { echo \"no device\" ; exit
      1 ; }\n\n\t# If a kernel device name change happens and a block device file\n\t#
      in the OSD directory becomes missing, this OSD fails to start\n\t# continuously.
      This problem can be resolved by confirming\n\t# the validity of the device file
      and recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
      [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ; then\n\t\trm
      $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports bluestore so
      we don't need to pass a store flag\n\tceph-volume raw activate --device \"$DEVICE\"
      --no-systemd --no-tmpfs\nfi\n"
    env:
    - name: CEPH_VOLUME_DEBUG
      value: "1"
    - name: CEPH_VOLUME_SKIP_RESTORECON
      value: "1"
    - name: DM_DISABLE_UDEV
      value: "1"
    - name: ROOK_CEPH_MON_HOST
      valueFrom:
        secretKeyRef:
          key: mon_host
          name: rook-ceph-config
    - name: CEPH_ARGS
      value: -m $(ROOK_CEPH_MON_HOST)
    - name: ROOK_BLOCK_PATH
      value: /dev/sde
    - name: ROOK_METADATA_DEVICE
    - name: ROOK_WAL_DEVICE
    - name: ROOK_OSD_ID
      value: "31"
    envFrom:
    - configMapRef:
        name: rook-ceph-osd-env-override
        optional: true
    image: quay.io/ceph/ceph:v17.2.6
    imagePullPolicy: IfNotPresent
    name: activate
    resources:
      limits:
        memory: 24Gi
      requests:
        cpu: "1"
        memory: 18Gi
    securityContext:
      privileged: true
      runAsUser: 0
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/lib/ceph/osd/ceph-31
      name: activate-osd
    - mountPath: /dev
      name: devices
    - mountPath: /etc/temp-ceph
      name: rook-config-override
      readOnly: true
    - mountPath: /etc/ceph/admin-keyring-store/
      name: rook-ceph-admin-keyring
      readOnly: true
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-p7ngl
      readOnly: true
  - args:
    - bluefs-bdev-expand
    - --path
    - /var/lib/ceph/osd/ceph-31
    command:
    - ceph-bluestore-tool
    image: quay.io/ceph/ceph:v17.2.6
    imagePullPolicy: IfNotPresent
    name: expand-bluefs
    resources:
      limits:
        memory: 24Gi
      requests:
        cpu: "1"
        memory: 18Gi
    securityContext:
      privileged: true
      runAsUser: 0
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/lib/ceph/osd/ceph-31
      name: activate-osd
    - mountPath: /dev
      name: devices
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-p7ngl
      readOnly: true
  - args:
    - --verbose
    - --recursive
    - ceph:ceph
    - /var/log/ceph
    - /var/lib/ceph/crash
    - /run/ceph
    command:
    - chown
    image: quay.io/ceph/ceph:v17.2.6
    imagePullPolicy: IfNotPresent
    name: chown-container-data-dir
    resources:
      limits:
        memory: 24Gi
      requests:
        cpu: "1"
        memory: 18Gi
    securityContext:
      privileged: true
      readOnlyRootFilesystem: false
      runAsUser: 0
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/lib/rook
      name: rook-data
    - mountPath: /etc/ceph
      name: rook-config-override
      readOnly: true
    - mountPath: /run/ceph
      name: ceph-daemons-sock-dir
    - mountPath: /var/log/ceph
      name: rook-ceph-log
    - mountPath: /var/lib/ceph/crash
      name: rook-ceph-crash
    - mountPath: /dev
      name: devices
    - mountPath: /run/udev
      name: run-udev
    - mountPath: /var/lib/ceph/osd/ceph-31
      name: activate-osd
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-p7ngl
      readOnly: true
  nodeName: sc-node-ceph-4
  nodeSelector:
    kubernetes.io/hostname: sc-node-ceph-4
  preemptionPolicy: PreemptLowerPriority
  priority: 2000001000
  priorityClassName: system-node-critical
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: rook-ceph-osd
  serviceAccountName: rook-ceph-osd
  shareProcessNamespace: true
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.rook.io/cephfs
    operator: Exists
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - hostPath:
      path: /var/lib/rook
      type: ""
    name: rook-data
  - name: rook-config-override
    projected:
      defaultMode: 420
      sources:
      - configMap:
          items:
          - key: config
            mode: 292
            path: ceph.conf
          name: rook-config-override
  - hostPath:
      path: /var/lib/rook/exporter
      type: DirectoryOrCreate
    name: ceph-daemons-sock-dir
  - hostPath:
      path: /var/lib/rook/rook-ceph/log
      type: ""
    name: rook-ceph-log
  - hostPath:
      path: /var/lib/rook/rook-ceph/crash
      type: ""
    name: rook-ceph-crash
  - hostPath:
      path: /dev
      type: ""
    name: devices
  - hostPath:
      path: /run/udev
      type: ""
    name: run-udev
  - hostPath:
      path: /var/lib/rook/rook-ceph/aa0e7bed-d3e3-49a7-a471-8e354bfe61f6_c803a2ec-8f5c-4b23-b1d1-1319ea2fd989
      type: DirectoryOrCreate
    name: activate-osd
  - name: rook-ceph-admin-keyring
    secret:
      defaultMode: 420
      secretName: rook-ceph-admin-keyring
  - name: kube-api-access-p7ngl
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2024-01-23T06:59:30Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2024-03-08T02:13:38Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2024-03-08T02:13:38Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2024-01-23T06:58:37Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: containerd://521fbec2ecc53e08312139568a32e128575f03cc4c257e72c0a17eed6bd1e48a
    image: quay.io/ceph/ceph:v17.2.6
    imageID: quay.io/ceph/ceph@sha256:6b0a24e3146d4723700ce6579d40e6016b2c63d9bf90422653f2d4caa49be232
    lastState: {}
    name: log-collector
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2024-01-23T06:59:30Z"
  - containerID: containerd://9ace84408a1abb2cfe2a18d6f2c28e6b749db0f99cabf017c4a7d14d8dd618d7
    image: quay.io/ceph/ceph:v17.2.6
    imageID: quay.io/ceph/ceph@sha256:6b0a24e3146d4723700ce6579d40e6016b2c63d9bf90422653f2d4caa49be232
    lastState:
      terminated:
        containerID: containerd://7161c617eebb92cdc2a0beaeb914068837f5c3899dbc57f2681d8a24dd3c99fa
        exitCode: 0
        finishedAt: "2024-03-08T02:12:49Z"
        reason: Completed
        startedAt: "2024-03-04T15:03:51Z"
    name: osd
    ready: true
    restartCount: 9
    started: true
    state:
      running:
        startedAt: "2024-03-08T02:12:51Z"
  hostIP: 172.70.21.25
  initContainerStatuses:
  - containerID: containerd://68001d51682641c4c2d0d13f3601a0cf07e9118b717cd4929e07038b88a17bd5
    image: quay.io/ceph/ceph:v17.2.6
    imageID: quay.io/ceph/ceph@sha256:6b0a24e3146d4723700ce6579d40e6016b2c63d9bf90422653f2d4caa49be232
    lastState: {}
    name: activate
    ready: true
    restartCount: 0
    state:
      terminated:
        containerID: containerd://68001d51682641c4c2d0d13f3601a0cf07e9118b717cd4929e07038b88a17bd5
        exitCode: 0
        finishedAt: "2024-01-23T06:58:40Z"
        reason: Completed
        startedAt: "2024-01-23T06:58:38Z"
  - containerID: containerd://0bd242b9dc34cc02277ca7cb617ffa52f6fdd6ce260895ce144746780fcb60dc
    image: quay.io/ceph/ceph:v17.2.6
    imageID: quay.io/ceph/ceph@sha256:6b0a24e3146d4723700ce6579d40e6016b2c63d9bf90422653f2d4caa49be232
    lastState: {}
    name: expand-bluefs
    ready: true
    restartCount: 0
    state:
      terminated:
        containerID: containerd://0bd242b9dc34cc02277ca7cb617ffa52f6fdd6ce260895ce144746780fcb60dc
        exitCode: 0
        finishedAt: "2024-01-23T06:59:28Z"
        reason: Completed
        startedAt: "2024-01-23T06:58:41Z"
  - containerID: containerd://6b885130d490121abf72abd9664874ae6c402c1c0e044656fed55660995bc81f
    image: quay.io/ceph/ceph:v17.2.6
    imageID: quay.io/ceph/ceph@sha256:6b0a24e3146d4723700ce6579d40e6016b2c63d9bf90422653f2d4caa49be232
    lastState: {}
    name: chown-container-data-dir
    ready: true
    restartCount: 0
    state:
      terminated:
        containerID: containerd://6b885130d490121abf72abd9664874ae6c402c1c0e044656fed55660995bc81f
        exitCode: 0
        finishedAt: "2024-01-23T06:59:29Z"
        reason: Completed
        startedAt: "2024-01-23T06:59:29Z"
  phase: Running
  podIP: 172.70.21.25
  podIPs:
  - ip: 172.70.21.25
  qosClass: Burstable
  startTime: "2024-01-23T06:58:37Z"
